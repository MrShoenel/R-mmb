<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Hyperparameter Evaluation • mmb</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha256-916EbMg70RQy9LHiGkXzG8hSg9EdNy97GazNG/aiY1w=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha256-U5ZEeKfGNOja007MMD3YBI0A3OSZOQbeG6z2f2Y0hu8=" crossorigin="anonymous"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.1/css/all.min.css" integrity="sha256-nAmazAk6vS34Xqo0BSrTb+abbtFlgsFK7NKSi6o7Y78=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.1/css/v4-shims.min.css" integrity="sha256-6qHlizsOWFskGlwVOKuns+D1nB6ssZrHQrNj1wGplHc=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" integrity="sha256-FiZwavyI2V6+EXO1U+xzLG3IKldpiTFf3153ea9zikQ=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.9.4/headroom.min.js" integrity="sha256-DJFC1kqIhelURkuza0AvYal5RxMtpzLjFhsnVIeuk+U=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.9.4/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Hyperparameter Evaluation">
<meta property="og:description" content="">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">mmb</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.11.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fas fa fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/Hyperparameter-Evaluation.html">Hyperparameter Evaluation</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1>Hyperparameter Evaluation</h1>
                        <h4 class="author">Sebastian Hönel</h4>
            
            <h4 class="date">2020-03-16</h4>
      
      
      <div class="hidden name"><code>Hyperparameter-Evaluation.rmd</code></div>

    </div>

    
    
<blockquote>
<p>This vignette only embeds a rendered version of another <code>R</code> notebook, that evaluates the effect of hyperparameters of all functions used for inferencing in the package <strong><code>mmb</code></strong>. It should be displayed right below this paragraph. If you have trouble viewing it, check out the latest version on Github: <a href="https://mrshoenel.github.io/R-mmb/articles/hyperparameters.html">Hyperparameter Evaluation</a>. Additional formats are available here: <a href="https://github.com/MrShoenel/R-mmb/tree/master/eval/">Additional Formats</a>.</p>
</blockquote>
<hr>
<p>Functions implemented in <strong><code>mmb</code></strong> support some hyperparameters. This notebook evaluates them and compares performance to other well-known methods, using some well-known methods. The results of this notebook are included in the vignette <em>Hyperparameter Evaluation</em>. Since this is a rather substantial analysis, it cannot be a vignette itself, and is built externally.</p>
<div id="grid-search" class="section level1">
<h1 class="hasAnchor">
<a href="#grid-search" class="anchor"></a>Grid search</h1>
<p>In the first section, we are evaluating our new classifier and regressor for some well known datasets, using a grid-search approach and <code>caret</code>. We start with classification, then continue with regression. Those best parameters will then be used in further comparisons against other models.</p>
<p>In the following block, we can globally disable attempts to compute results. If the resulting files are present, then the related evaluations are skipped anyway. However, setting the following to FALSE, will prevent the actual attempt.</p>
<pre><code>DISABLE_COMPUTATIONS &lt;- TRUE
# Used with many qualititave values as brewer color palette:
QPALETTE &lt;- "Set3"
# This is in inches!
tikzWidthFull &lt;- 6.1 # 6.12619 original/maximal for JSS article</code></pre>
<p>Let’s first install all the needed packages:</p>
<p>First, define the models we want to test:</p>
<pre><code># Used by both regression and classification!
caret_models &lt;- c(
  "null", "bayesCaret", "ranger", "gbm", "mlpML", "bayesglm")

# This is extra models used for classification/regression resp.:
caret_models_c &lt;- c(caret_models,
  "pam", "naive_bayes", "C5.0", "rda")
caret_models_r &lt;- c(caret_models,
  "knn", "pcr", "kernelpls", "rpart2")</code></pre>
<p>We define a couple of helper-functions:</p>
<pre><code># We will truncate all datasets to this amount if larger using
# appropriate resampling!
max_rows_dataset &lt;- 2000


#' Unifies the way we retrieve a dataset. Already splits the dataset
#' into training- and validation partitions. The split is 0.7, if the
#' amount of rows is less than 800; 0.85, otherwise.
#' @param name a short-cut name of the dataset to obtain, e.g. "iris"
#' @param seedMult an integer used for \code{set.seed()}. The seedMult
#' influences how we sample into training and validation.
#' @param useDs an optional data.frame of a 3rd party dataset. Use this
#' to use your own dataset but still obtain a list as produced by this
#' function.
#' @param useLabel character the name of the label to use when passing
#' in an own dataset using 'useDs'.
#' @return list with the entries 'ds' (the entire dataset as data.frame),
#' 'label' (name of column to be the label/target), 'train' and 'valid'
#' (data.frames with training and validation data; train+valid = ds)
get_dataset &lt;- function(name, seedMult, useDs = NULL, useLabel = NULL) {
  set.seed((0xceed * seedMult * max_rows_dataset) %% .Machine$integer.max)
  
  ds &lt;- NULL
  dsLabel &lt;- "Class"

  if (name == "chick") {
    data("ChickWeight")
    ds &lt;- data.frame(ChickWeight)
    dsLabel &lt;- "Diet"
  } else if (name == "iris") {
    data("iris")
    ds &lt;- iris
    dsLabel &lt;- "Species"
  } else if (name == "credit") {
    data("GermanCredit")
    ds &lt;- data.frame(GermanCredit)[, 1:10]
  } else if (name == "glass") {
    data("Glass")
    ds &lt;- data.frame(Glass)
    dsLabel &lt;- "Type"
  } else if (name == "breast") {
    data("BreastCancer")
    ds &lt;- data.frame(BreastCancer)
    
    ds$Id &lt;- NULL
    ds$Cl.thickness &lt;- as.integer(ds$Cl.thickness)
    ds$Cell.size &lt;- as.integer(ds$Cell.size)
    ds$Cell.shape &lt;- as.integer(ds$Cell.shape)
    ds$Marg.adhesion &lt;- as.integer(ds$Marg.adhesion)
    ds$Epith.c.size &lt;- as.integer(ds$Epith.c.size)
    ds$Bare.nuclei &lt;- as.integer(ds$Bare.nuclei)
    ds$Bl.cromatin &lt;- as.integer(ds$Bl.cromatin)
    ds$Normal.nucleoli &lt;- as.integer(ds$Normal.nucleoli)
    ds$Mitoses &lt;- as.integer(ds$Mitoses)
  } else if (name == "sonar") {
    data("Sonar")
    ds &lt;- data.frame(Sonar)
  } else if (name == "vowel") {
    data("Vowel")
    ds &lt;- data.frame(Vowel)
    ds$V1 &lt;- as.integer(ds$V1)
  } else if (name == "iono") {
    data("Ionosphere")
    ds &lt;- data.frame(Ionosphere)
    ds$V1 &lt;- as.integer(ds$V1)
    ds$V2 &lt;- NULL
  } else if (name == "vehicle") {
    data("Vehicle")
    ds &lt;- data.frame(Vehicle)
  } else if (name == "letter") {
    data("LetterRecognition")
    ds &lt;- data.frame(LetterRecognition)
    dsLabel &lt;- "lettr"
  } else if (name == "satellite") {
    data("Satellite")
    ds &lt;- data.frame(Satellite)
    dsLabel &lt;- "classes"
  } else if (name == "segment") {
    data("segmentationData")
    ds &lt;- data.frame(segmentationData)
    ds$Cell &lt;- NULL
    ds$Case &lt;- as.integer(ds$Case)
  }
  
  
  # Below regression-datasets!
  # 'label'/'dsLabel' is the target regression column!
  else if (name == "diamonds") {
    data("diamonds")
    ds &lt;- data.frame(diamonds)
    dsLabel &lt;- "price"
    
    ds$cut &lt;- as.integer(ds$cut)
    ds$color &lt;- as.integer(ds$color)
    ds$clarity &lt;- as.integer(ds$clarity)
  } else if (name == "sacramento") {
    data("Sacramento")
    ds &lt;- data.frame(Sacramento)
    dsLabel &lt;- "price"
    
    ds$zip &lt;- NULL
    ds$city &lt;- as.character(ds$city)
    ds$type &lt;- as.character(ds$type)
  } else if (name == "boston") {
    data("BostonHousing")
    ds &lt;- data.frame(BostonHousing)
    dsLabel &lt;- "ptratio"
    ds$chas &lt;- as.character(ds$chas)
  } else if (name == "midwest") {
    data("midwest")
    ds &lt;- data.frame(midwest)
    dsLabel &lt;- "percwhite"
    
    ds$PID &lt;- NULL
    ds$county &lt;- NULL
    ds$category &lt;- NULL
  } else if (name == "seals") {
    data("seals")
    ds &lt;- data.frame(seals)
    dsLabel &lt;- "delta_long"
  } else if (name == "txhousing") {
    data("txhousing")
    ds &lt;- data.frame(txhousing)
    dsLabel &lt;- "sales"
  } else if (name == "mortality") {
    data("USRegionalMortality")
    ds &lt;- data.frame(USRegionalMortality)
    dsLabel &lt;- "Rate"
    
    ds$Region &lt;- as.character(ds$Region)
    ds$Status &lt;- as.character(ds$Status)
    ds$Sex &lt;- as.character(ds$Sex)
    ds$Cause &lt;- as.character(ds$Cause)
  } else if (name == "singer") {
    data("singer")
    ds &lt;- data.frame(singer)
    dsLabel &lt;- "height"
    ds$voice.part &lt;- as.character(ds$voice.part)
  } else if (name == "pima") {
    data("PimaIndiansDiabetes")
    ds &lt;- data.frame(PimaIndiansDiabetes)
    dsLabel &lt;- "age"
    ds$diabetes &lt;- as.character(ds$diabetes)
  } else if (name == "ozone") {
    data("Ozone")
    ds &lt;- data.frame(Ozone)
    dsLabel &lt;- "V11"
    
    ds$V9 &lt;- NULL # too many NAs
    ds$V1 &lt;- as.integer(ds$V1)
    ds$V2 &lt;- as.integer(ds$V2)
    ds$V3 &lt;- as.integer(ds$V3)
  } else if (name == "faithful") {
    data("faithfuld")
    ds &lt;- data.frame(faithfuld)
    dsLabel &lt;- "density"
  }
  
  
  
  # Below non-datasets
  else if (is.data.frame(useDs)) {
    ds &lt;- useDs
    dsLabel &lt;- useLabel
  } else {
    stop(paste("Dataset is not known:", name))
  }
  
  # Only take complete rows!
  ds &lt;- ds[complete.cases(ds), ]
  
  if (nrow(ds) &gt; max_rows_dataset) {
    tempPart &lt;- createDataPartition(
      ds[[dsLabel]], p = max_rows_dataset / nrow(ds), list = F)
    ds &lt;- ds[tempPart, ]
  }
  
  part &lt;- createDataPartition(
    ds[[dsLabel]], p = ifelse(nrow(ds) &lt; 800, 0.7, 0.85), list = F)
  
  return(list(
    ds = ds,
    label = dsLabel,
    train = ds[part, ],
    valid = ds[-part, ]
  ))
}


#' Uses caret's 'bestTune', i.e., the best found hyperparameters from the
#' grid-search, and extracts the results from each fold/repeat that uses
#' these. The result is a table with the results obtained using the best
#' set of hyperparameters.
#' @param resample data.frame as obtained from a fitted model trained
#' with caret (model$resample). This should ideally be obtained using a
#' train-configuration where resample="all".
#' @param bestTune data.frame as obtained from a fitted model trained
#' with caret (model$bestTune).
#' @return data.frame a subset of 'resample', where the rows use the hyper-
#' parameters from 'bestTune'.
aggregate_caret_results &lt;- function(resample, bestTune) {
  conds &lt;- c()
  for (cName in colnames(bestTune)) {
    bt &lt;- bestTune[[cName]]
    cond &lt;- NULL
    wasEmpty &lt;- is.na(bt) || is.nan(bt)
    if (wasEmpty) {
      n &lt;- if (is.nan(bt)) "n" else ""
      cond &lt;- paste(
        "is.na", n, "(resample$", cName, ")", sep = "")
    } else if (is.numeric(bt) &amp;&amp; (bt[1] %% 1 != 0)) {
      cond &lt;- paste(
        "(abs(resample$", cName, " - ", "bestTune$", cName, ") &lt; 1e-15)", sep = "")
    } else if (is.character(bt) || is.factor(bt)) {
      cond &lt;- paste(
        "resample$", cName, " == ",
        paste('"', bestTune[[cName]], '"', sep = ""),
        sep = ""
      )
    } else {
      cond &lt;- paste(
        "resample$", cName, " == ", "bestTune$", cName, sep = "")
    }

    conds &lt;- c(conds, cond)
    if (!wasEmpty) {
      conds &lt;- c(conds, paste("!is.na(resample$", cName, ")", sep = ""))
    }
  }
  
  return(resample[eval(parse(text = paste(conds, collapse = " &amp; "))), ])
}


#' Returns a list of seeds used in parallel training with caret. For
#' repeatability, we need deterministic seeds. The amount depends on
#' the amounts of hyperparamenters, and number of folds/repeats.
#' @param nh integer, the number of hyperparameters
#' @param amount integer, the number of seeds, usually this is number
#' of folds * number of repeats.
#' @param seed integer used in \code{set.seed()}. Given an identical
#' seed, this function produces the same seeds (idempotent).
#' @return list with seeds that can be used in caret's trainControl
get_seeds &lt;- function(nh, amount, seed = 42) {
  set.seed(seed)
  
  seeds &lt;- vector(mode = "list", length = amount + 1)
  for(i in 1:amount) seeds[[i]] &lt;- sample.int(.Machine$integer.max, nh)
  # For the last model:
  seeds[[amount + 1]] &lt;- sample.int(.Machine$integer.max, 1)
  return(seeds)
}</code></pre>
<p>Let’s define a function to perform grid search.</p>
<pre><code>#' modelsAndDatasets is a data.frame with columns 'model' and 'dataset'.
#' It should already match each model with the right dataset, i.e., it
#' should not have a row {nb,diamonds}, as Naive Bayes cannot regress.
#' This function does not return anything, but stores the result of each
#' model/dataset combination as CSV.
#' @param modelsAndDatasets data.frame, with the columns 'model' and
#' 'dataset'. Model needs to be acceptable for caret, and dataset needs
#' to be acceptable by @seealso \code{get_dataset()}.
#' @return void
get_other_models_results &lt;- function(modelsAndDatasets = data.frame()) {
  if (DISABLE_COMPUTATIONS) {
    return(0)
  }
  perms &lt;- modelsAndDatasets[order(modelsAndDatasets$model, modelsAndDatasets$dataset), ]
  set.seed(0xbeef42)
  perms &lt;- perms[sample(rownames(perms)), ]
  
  
  numCores &lt;- parallel::detectCores()
  numCoresNested &lt;- if (nrow(perms) &lt; numCores) numCores else max(2, numCores / 4)
  cl &lt;- parallel::makeCluster(max(2, numCores / 2), outfile = "")
  clusterExport(cl, as.list(names(.GlobalEnv)))
  registerDoSNOW(cl)
  pb &lt;- txtProgressBar(min = 0, max = nrow(perms), style = 3)
  
  
  caretModelResultsList &lt;- foreach(
    permRowIdx = rownames(perms),
    .packages = c(packages_required),
    .options.snow = list(
      progress = function(n) {
        writeLines(paste(n, Sys.time(), sep = "  --  "), con = "progress.txt")
        setTxtProgressBar(pb, n)
      }
    )
  ) %dopar% {
    idx &lt;- as.numeric(permRowIdx)
    permRow &lt;- perms[permRowIdx, ]
    dsList &lt;- get_dataset(permRow$dataset, seedMult = 1337 * idx)
    trP &lt;- nrow(dsList$train) / nrow(dsList$ds)
    isRegress &lt;- is.numeric(dsList$ds[, dsList$label])
    typeShort &lt;- if (isRegress) "r" else "c"
    isMMB &lt;- length(grep("bayesCaret", permRow$model)) &gt; 0
    
    dataName &lt;- paste("results_", typeShort, "/", permRow$model, "_", permRow$dataset, "_", typeShort, ".csv", sep = "")
    if (file.exists(dataName)) {
      return(read.csv(dataName))
    }
    
    # Workaround: The models mlpML, pam, rda do not support ordered factors
    # in the chick dataset and will fail, but we can convert them to numeric.
    if (permRow$model %in% c("mlpML", "pam", "rda") &amp;&amp; permRow$dataset == "chick") {
      dsList$ds$Chick &lt;- as.numeric(dsList$ds$Chick)
      dsList$train$Chick &lt;- as.numeric(dsList$train$Chick)
      dsList$valid$Chick &lt;- as.numeric(dsList$valid$Chick)
    }
    
    x &lt;- dsList$ds[, !(colnames(dsList$ds) %in% c(dsList$label))]
    y &lt;- dsList$ds[, dsList$label]
    
    rcvNumber &lt;- 5
    rcvRepeats &lt;- 3
    trMethod &lt;- "repeatedcv"
    tuneLength &lt;- ifelse(trMethod == "none", 1, 3)
    numHypers &lt;- nrow(if (isMMB) mmb::bayesCaret$grid(x = x, y = y) else caret::getModelInfo(
      permRow$model)[[1]]$grid(len = tuneLength, x = x, y = y))

    set.seed(idx * 23)
    trControl &lt;- caret::trainControl(
      method = trMethod, number = rcvNumber, repeats = rcvRepeats,
      p = trP, savePredictions = "all", returnResamp = "all",
      seeds = get_seeds(numHypers, rcvNumber * rcvRepeats, seed = idx * 23))
    
    clNested &lt;- makePSOCKcluster(numCoresNested)
    registerDoParallel(clNested)
    
    set.seed(idx * 371)
    timeStart &lt;- proc.time()[["elapsed"]]
    modelName &lt;- as.character(permRow$model)
    modelObj &lt;- if (isMMB) mmb::bayesCaret else modelName
    model &lt;- caret::train(x = x, y = y, method = modelObj, trControl = trControl)
    timeEnd &lt;- proc.time()[["elapsed"]]
    
    stopCluster(clNested)
    
    # Now we store 2 things:
    # - The full resample
    # - The aggregated resample (those matching bestTune)
    dataFull &lt;- model$resample
    dataFull$Model &lt;- permRow$model
    dataFull$Dataset &lt;- permRow$dataset
    dataFull$Runtime &lt;- (timeEnd - timeStart) / (rcvNumber * rcvRepeats)
    
    
    # Now the aggregated:
    dataAgg &lt;- aggregate_caret_results(dataFull, model$bestTune)
    
    
    # Rename some columns and write files:
    for (cn in colnames(model$bestTune)) {
      dataFull[[paste("hyper.", cn, sep = "")]] &lt;- dataFull[[cn]]
      if (!(cn %in% c("Model", "Dataset", "Runtime"))) {
        dataFull[[cn]] &lt;- NULL # delete renamed column
      }
      
      dataAgg[[paste("hyper.", cn, sep = "")]] &lt;- dataAgg[[cn]]
      if (!(cn %in% c("Model", "Dataset", "Runtime"))) {
        dataAgg[[cn]] &lt;- NULL
      }
    }
    
    
    dataName &lt;- paste("results_", typeShort, "/", permRow$model, "_", permRow$dataset, "_", typeShort, ".csv", sep = "")
    utils::write.csv(dataFull, dataName)
    dataName &lt;- paste("results_", typeShort, "/", permRow$model, "_", permRow$dataset, "_", typeShort, "_agg.csv", sep = "")
    utils::write.csv(dataAgg, dataName)
    
    
    return(dataFull)
  }
  
  stopCluster(cl)
  registerDoSEQ()
}</code></pre>
<div id="classification" class="section level2">
<h2 class="hasAnchor">
<a href="#classification" class="anchor"></a>Classification</h2>
<p>Here, we use the full grid as provided by <code>bayesCaret</code>. Please note that the grid and its size depend on whether we are doing classification or not.</p>
<pre><code>datasets_c &lt;- c("iris", "chick", "credit", "glass", "breast", "sonar", "iono", "vehicle")
modelAndData_c &lt;- expand.grid(
  model = caret_models_c,
  dataset = datasets_c,
  stringsAsFactors = FALSE)

get_other_models_results(modelAndData_c)

## [1] 0</code></pre>
</div>
<div id="regression" class="section level2">
<h2 class="hasAnchor">
<a href="#regression" class="anchor"></a>Regression</h2>
<pre><code># We set a new default regressor, to avoid NaN in sparse predictions.
mmb::setDefaultRegressor(function(data) {
  res &lt;- mmb::estimatePdf(data)$argmax
  if (is.na(res)) {
    res &lt;- median(data)
  }
  return(res)
})

datasets_r &lt;- c("mortality", "sacramento", "boston", "seals", "faithful", "diamonds", "pima", "txhousing")
modelAndData_r &lt;- expand.grid(
  model = caret_models_r,
  dataset = datasets_r,
  stringsAsFactors = FALSE)

get_other_models_results(modelAndData_r)

## [1] 0</code></pre>
<p>Let’s print some table/overview of datasets that we actually use.</p>
<pre><code>datasets_overview &lt;- do.call(rbind, lapply(c(datasets_c, datasets_r), function(d) {
  dsList &lt;- get_dataset(d, seedMult = 1)
  isReg &lt;- is.numeric(dsList$valid[[dsList$label]])
  
  return(data.frame(
    Name = d,
    NumTrain = nrow(dsList$train),
    NumValid = nrow(dsList$valid),
    Predictors = length(colnames(dsList$train)),
    Classes = if (isReg) "Regression" else length(unique(as.character(dsList$ds[[dsList$label]])))
  ))
}))

print(datasets_overview)

##          Name NumTrain NumValid Predictors    Classes
## 1        iris      105       45          5          3
## 2       chick      405      173          4          4
## 3      credit      850      150         10          2
## 4       glass      153       61         10          6
## 5      breast      479      204         10          2
## 6       sonar      146       62         61          2
## 7        iono      247      104         34          2
## 8     vehicle      722      124         19          4
## 9   mortality      281      119          6 Regression
## 10 sacramento      795      137          8 Regression
## 11     boston      356      150         14 Regression
## 12      seals      983      172          4 Regression
## 13   faithful     1701      300          3 Regression
## 14   diamonds     1703      299         10 Regression
## 15       pima      539      229          9 Regression
## 16  txhousing     1704      298          9 Regression</code></pre>
</div>
</div>
<div id="evaluation" class="section level1">
<h1 class="hasAnchor">
<a href="#evaluation" class="anchor"></a>Evaluation</h1>
<p>In this section, we will load the results and aggregate all data, so that we can then continue with visualizations and statistical analyses. We begin by first loading the results for all other models, before continuing with our Bayesian results.</p>
<pre><code>install.packages_cond("extrafont")

## Registering fonts with R

if (.Platform$OS.type == "windows") {
  windowsFonts(Times = windowsFont("Times"))
  windowsFonts(Consolas = windowsFont("Consolas"))
}
# Was overridden by RandomForest
margin &lt;- ggplot2::margin


#' Given a ggplot2 instance, saves the plot as EPS in the figures-directory,
#' and then returns it for further processing.
#' @param ggplotInstance a gg plot
#' @param fileName character the name of the file, without the extension.
#' @param width in inches, defaults to @seealso \code{tikzWidthFull}
#' @param height in inches, defaults to 6cm (~2.36in)
#' @return ggplot the instance as given, unchanged
saveAndPlotAsEPS &lt;- function(ggplotInstance, fileName, width = tikzWidthFull, height = 600/254) {
  f &lt;- "figures"
  if (!dir.exists(f)) {
    dir.create(f)
  }
  
  fileName &lt;- paste(f, paste(fileName, ".eps", sep = ""), sep = "/")
  ggsave(fileName, ggplotInstance,
         width = floor(width * 100) / 100,
         height = floor(height * 100) / 100,
         limitsize = F, device = cairo_pdf)
  return(ggplotInstance)
}</code></pre>
<div id="evaluation-of-the-bayesian-models-compared-to-other-models" class="section level2">
<h2 class="hasAnchor">
<a href="#evaluation-of-the-bayesian-models-compared-to-other-models" class="anchor"></a>Evaluation of the Bayesian models, compared to other models</h2>
<p>In this section, we will compare the performance of our Bayesian models to other, well-known models, such as Random forest.</p>
<div id="load-and-aggregate-the-data-classification" class="section level3">
<h3 class="hasAnchor">
<a href="#load-and-aggregate-the-data-classification" class="anchor"></a>Load and aggregate the data (classification)</h3>
<p>We need to aggregate the results from each model, both for classification and regression. While <code>caret</code> determined a <em>best fit</em> per model (these are the results in the <code>*_agg.csv</code>-files), we will manually determine those for <code>BayesCaret</code>, as the result between full/simple/naive are quite different.</p>
<p>We use the following naming-schema for our results:</p>
<ul>
<li>
<code>gridresults_{type}_{model(s)}</code>, where <code>type</code> is either ‘r’ or ‘c’, and <code>model</code> is either ‘bc_{f/s/n}’ (BayesCaret full/simple/naive <span class="math display">\[only used in the ‘Model’ column, the name of the variable will
only be ‘bc’\]</span>) or ‘other’ (all other models)</li>
<li>
<code>bestHps_{type}_{model(s)}</code>, takes the best performing hyperparameters for each model/dataset combination and filters the grid-results. Similar to taking <em>caret</em>’s <code>bestTune</code> and returning those rows that match the hyperparameters of it.</li>
</ul>
<p>We keep the gridresults for BayesCaret and all other models separate, as we keep the full hyperparameters for our models for later evaluation. For other models, we only report their performance regardless of the hyperparameters used by caret in the grid-search.</p>
<pre><code>#' Reads all BayesCaret-datasets, merges complete cases and returns them.
#' @param d the name of the dataset, e.g. "iris"
#' @param type either "c" or "r"
#' @return data.frame of all merged CSV files (only complete cases).
aggregateBCdatasets &lt;- function(d, type) {
  cc &lt;- if (type == "c") c("Accuracy", "Kappa") else c("RMSE", "Rsquared")
  csv &lt;- read.csv(paste("results_", type, "/bayesCaret_", d, "_", type, ".csv", sep = ""))
  csv &lt;- csv[complete.cases(csv[, cc]), ]
  
  # Remove X column
  csv$X &lt;- NULL
  
  # When Naive Bayes was added, simple=T/F was replaced by mode=simple/full/naive.
  # When reading in old results that do not have naive, we engineer this column
  # and delete the 'simple'-column.
  if (!("hyper.mode" %in% colnames(csv))) {
    csv$hyper.mode &lt;- sapply(csv$hyper.simple, function(x) if (x) "simple" else "full")
    csv$hyper.simple &lt;- NULL
  }
  
  # Set Model to 'bc_{f/s/n}'
  csv$Model &lt;- sapply(csv$hyper.mode, function(m) paste("bc",
    if (m == "simple") "s" else if (m == "full") "f" else "n", sep = "_"))
  
  return(csv)
}

#' Finds the best tune per model (full/simple/naive) and model, and extracts
#' the results from all resamples matching the hyperparameters from the best tune.
#' @param gridresults data.frame with all results from the grid-search
#' @param type either "c" or "r"
#' @return data.frame with subset of best results
findBestBCtune &lt;- function(gridresults, type) {
  metric &lt;- if (type == "c") "Accuracy" else "RMSE"
  metrics &lt;- if (type == "c") c("Accuracy", "Kappa") else c("RMSE", "Rsquared", "MAE")
  # For each of these, we'll find a best-tune per dataset
  bcModels &lt;- c("bc_f", "bc_s", "bc_n")
  datasets &lt;- unique(as.character(gridresults$Dataset))
  
  bestResults &lt;- NULL
  
  for (ds in datasets) {
    for (m in bcModels) {
      dsTemp &lt;- gridresults[gridresults$Model == m &amp; gridresults$Dataset == ds, ]
      bestTuneFunc &lt;- if (type == "c") which.max else which.min
      bestTune &lt;- dsTemp[bestTuneFunc(dsTemp[[metric]]), ]
      
      best &lt;- aggregate_caret_results(
        dsTemp, bestTune = bestTune[1, grep("^hyper\\.", colnames(bestTune)), ])
      
      if (nrow(best) == 0) next
      
      # Add some more aggregates:
      for (ms in metrics) {
        best[[paste(ms, "min", sep = ".")]] &lt;- min(best[[ms]])
        best[[paste(ms, "max", sep = ".")]] &lt;- max(best[[ms]])
        best[[paste(ms, "mean", sep = ".")]] &lt;- mean(best[[ms]])
        best[[paste(ms, "median", sep = ".")]] &lt;- median(best[[ms]])
      }
      
      bestResults &lt;- rbind(bestResults, best)
    }
  }
  
  return(bestResults)
}

gridresults_c_bc &lt;- do.call(
  rbind, lapply(datasets_c, function(d) aggregateBCdatasets(d, "c")))
gridresults_r_bc &lt;- do.call(
  rbind, lapply(datasets_r, function(d) aggregateBCdatasets(d, "r")))

bestHps_c_bc &lt;- findBestBCtune(gridresults_c_bc, "c")
bestHps_r_bc &lt;- findBestBCtune(gridresults_r_bc, "r")</code></pre>
<p>Let’s aggregate all the other datasets into one common data.frame while we’re at it:</p>
<pre><code>#' Merges and aggregates all other datasets, similar to the function
#' that does the same thing for BayesCaret.
#' @param d character name of dataset, e.g. "iris"
#' @param m character name of model, e.g. "ranger"
#' @param type "r" or "c"
#' @param agg BOOLEAN if true, aggregates the already aggregated
#' results (from best-tune as done by caret during the evaluation;
#' these results are stored in CSVs with same name, ending in "_agg").
aggregateOtherdatasets &lt;- function(d, m, type, agg = FALSE) {
  aggSuff &lt;- if (agg) "_agg" else ""
  f &lt;- paste("results_", type, "/", m, "_", d, "_", type, aggSuff, ".csv", sep = "")
  if (!file.exists(f)) return(data.frame()) # Some may be still computing..
  
  # Let's read the results for every model, but only keep the metrics's columns.
  keepCols &lt;- c("Dataset", "Model", "Accuracy", "Kappa", "RMSE", "Rsquared", "MAE", "Resample", "Runtime")
  
  cc &lt;- if (type == "c") c("Accuracy", "Kappa") else c("RMSE") # "Rsquared" is NA for "null" (ZeroR)
  csv &lt;- read.csv(f)
  csv &lt;- csv[complete.cases(csv[, cc]), ]
  
  if (m == "null" &amp;&amp; type == "r") {
    csv$Rsquared &lt;- 0
  }
  
  # Remove X column
  csv$X &lt;- NULL
  
  return(csv[, colnames(csv) %in% keepCols])
}

gridresults_c_others &lt;- do.call(
  rbind, lapply(datasets_c, function(d) do.call(rbind, lapply(caret_models_c, function(m) {
    if (m == "bayesCaret") return(data.frame()) # Effectively skip this
    return(aggregateOtherdatasets(d, m, "c"))
  }))))

gridresults_r_others &lt;- do.call(
  rbind, lapply(datasets_r, function(d) do.call(rbind, lapply(caret_models_r, function(m) {
    if (m == "bayesCaret") return(data.frame())
    return(aggregateOtherdatasets(d, m, "r"))
  }))))

bestHps_c_others &lt;- do.call(
  rbind, lapply(datasets_c, function(d) do.call(rbind, lapply(caret_models_c, function(m) {
    if (m == "bayesCaret") return(data.frame()) # Effectively skip this
    return(aggregateOtherdatasets(d, m, "c", agg = TRUE))
  }))))

bestHps_r_others &lt;- do.call(
  rbind, lapply(datasets_r, function(d) do.call(rbind, lapply(caret_models_r, function(m) {
    if (m == "bayesCaret") return(data.frame())
    return(aggregateOtherdatasets(d, m, "r", agg = TRUE))
  }))))</code></pre>
</div>
<div id="plot-results-for-accuracy-and-kappa" class="section level3">
<h3 class="hasAnchor">
<a href="#plot-results-for-accuracy-and-kappa" class="anchor"></a>Plot results for Accuracy and Kappa</h3>
<p>We show some plots that our classifier was run on, showing Accuracy and Kappa for full- and simple-models.</p>
<pre><code>(ggplot(gridresults_c_bc, aes(x=Dataset, y=Accuracy, color=Dataset, fill=Dataset)) +
  labs(subtitle = "Accuracy of full, naive and simple classification.") +
  geom_boxplot(color="black") +
  facet_wrap(hyper.mode ~ ., labeller = label_both) +
  theme_light(base_size = 9) +
  scale_color_brewer(palette = QPALETTE) +
  scale_fill_brewer(palette = QPALETTE) +
  ylim(0, 1) +
  theme(
    text = element_text(family="Consolas"),
    axis.text.x = element_text(angle = 45, margin = margin(t=10), hjust = .66),
    axis.title.x = element_blank(), strip.background = element_rect(fill="#dfdfdf"),
    axis.title.y = element_text(margin = margin(r=10)),
    legend.text = element_text(size = 8),
    strip.text = element_text(color="black"))
) %&gt;% saveAndPlotAsEPS("Bayes-all-datasets_Acc")</code></pre>
<p><img src="hyperparameters_files/figure-markdown_strict/unnamed-chunk-12-1.png"></p>
<pre><code># Now let's do the same, for Kappa
(ggplot(gridresults_c_bc, aes(x=Dataset, y=Kappa, color=Dataset, fill=Dataset)) +
  labs(subtitle = "Kappa of full, naive and simple classification.") +
  geom_boxplot(color="black") +
  facet_wrap(hyper.mode ~ ., labeller = label_both) +
  theme_light(base_size = 9) +
  scale_color_brewer(palette = QPALETTE) +
  scale_fill_brewer(palette = QPALETTE) +
  ylim(-.1, 1) + # Some results have a negative Kappa..
  theme(
    text = element_text(family="Consolas"),
    axis.text.x = element_text(angle = 45, margin = margin(t=10), hjust = .66),
    axis.title.x = element_blank(), strip.background = element_rect(fill="#dfdfdf"),
    axis.title.y = element_text(margin = margin(r=10)),
    legend.text = element_text(size = 8),
    strip.text = element_text(color="black"))
) %&gt;% saveAndPlotAsEPS("Bayes-all-datasets_Kappa")

## Warning: Removed 307 rows containing non-finite values (stat_boxplot).

## Warning: Removed 307 rows containing non-finite values (stat_boxplot).</code></pre>
<p><img src="hyperparameters_files/figure-markdown_strict/unnamed-chunk-12-2.png"></p>
</div>
<div id="plot-accuracy-and-kappa-as-compared-to-other-classifiers" class="section level3">
<h3 class="hasAnchor">
<a href="#plot-accuracy-and-kappa-as-compared-to-other-classifiers" class="anchor"></a>Plot Accuracy and Kappa as compared to other Classifiers</h3>
<p>Let’s do almost the same, but this time compare to all other trained models as well.</p>
<pre><code>tempDsCols_c &lt;- c("Dataset", "Model", "Accuracy", "Kappa", "Resample", "Runtime")
gridresults_c_all &lt;- rbind(
    gridresults_c_bc[, tempDsCols_c],
    gridresults_c_others[, tempDsCols_c])

# Rename some models:
gridresults_c_all$Model &lt;- sapply(
  as.character(gridresults_c_all$Model), function(m) if (m == "bc_f") "mmb:Full" else m)
gridresults_c_all$Model &lt;- sapply(
  as.character(gridresults_c_all$Model), function(m) if (m == "bc_n") "mmb:Naive" else m)
gridresults_c_all$Model &lt;- sapply(
  as.character(gridresults_c_all$Model), function(m) if (m == "bc_s") "mmb:Simple" else m)
gridresults_c_all$Model &lt;- sapply(
  as.character(gridresults_c_all$Model), function(m) if (m == "null") "ZeroR" else m)

# Next, we want to order the models:
modelsFirst_c &lt;- c("ZeroR", "mmb:Simple", "mmb:Full", "mmb:Naive")
modelsFirst_c &lt;- c(
  modelsFirst_c, sort(setdiff(unique(as.character(gridresults_c_all$Model)), modelsFirst_c)))
gridresults_c_all$Model &lt;- factor(
  as.character(gridresults_c_all$Model), levels = modelsFirst_c, ordered = TRUE)


facettedBoxplot &lt;- function(dataset, metric, metricName = metric, scales = "fixed", scale_y = scale_y_continuous()) {
  ggplot(dataset, aes(y = dataset[[metric]], x = Model, fill = Model)) +
    labs(subtitle = paste("Comparison of", metricName, "against the baseline (ZeroR) and other models.")) +
    geom_boxplot(lwd = .2,
                 outlier.colour = "#666666",
                 outlier.size = 2,
                 outlier.shape = 16,
                 outlier.alpha = .75,
                 outlier.stroke = 0) +
    coord_flip() +
    facet_wrap(Dataset ~., nrow = 2, scales = scales) +
    scale_y +
    scale_color_brewer(palette = QPALETTE) +
    scale_fill_brewer(palette = QPALETTE) +
    labs(fill = "Method") +
    theme_light(base_size = 9) +
    theme(plot.subtitle = element_text(size = 7, margin = margin(b=5)),
          text = element_text(family="Consolas"),
          axis.text.y = element_text(margin = margin(r=5)),
          axis.title.y.left = element_blank(),
          axis.text.x = element_text(angle = 90, margin = margin(t=5), hjust = 1, vjust = .4),
          axis.title.x = element_blank(),
          legend.margin = margin(l=5),
          legend.text = element_text(size = 7),
          legend.key.height = unit(14, "pt"),
          strip.background = element_rect(fill="#dfdfdf"),
          strip.text = element_text(color="black", size = 8))
}</code></pre>
<p>Let’s make some combined Box-plots:</p>
<pre><code>facettedBoxplot(gridresults_c_all, "Accuracy") %&gt;% saveAndPlotAsEPS("Bayes-all-datasets_bp_Accuracy", height = 3.2)</code></pre>
<p><img src="hyperparameters_files/figure-markdown_strict/unnamed-chunk-14-1.png"></p>
<pre><code>facettedBoxplot(gridresults_c_all, "Kappa") %&gt;% saveAndPlotAsEPS("Bayes-all-datasets_bp_Kappa", height = 3.2)</code></pre>
<p><img src="hyperparameters_files/figure-markdown_strict/unnamed-chunk-14-2.png"></p>
</div>
<div id="load-the-data-for-regression" class="section level3">
<h3 class="hasAnchor">
<a href="#load-the-data-for-regression" class="anchor"></a>Load the data for regression</h3>
<p>Almost the same as for classification, only that of the time of writing, not all results were present, so we add a check. Also, we already read all other results, as we will be needing <code>ZeroR</code>-results already.</p>
<p>Now, for each dataset, we want to show the performance metrics (MAE, RMSE, R^2) of BayesCaret (full and simple modes) vs. ZeroR. For that, we need to create a common dataset first:</p>
<pre><code>tempDsCols_r &lt;- c("Dataset", "Model", "RMSE", "Rsquared", "MAE", "Resample", "Runtime")
gridresults_r_all &lt;- rbind(
    gridresults_r_bc[, tempDsCols_r],
    gridresults_r_others[, tempDsCols_r])

# Rename some models:
gridresults_r_all$Model &lt;- sapply(
  as.character(gridresults_r_all$Model), function(m) if (m == "bc_f") "mmb:Full" else m)
gridresults_r_all$Model &lt;- sapply(
  as.character(gridresults_r_all$Model), function(m) if (m == "bc_s") "mmb:Simple" else m)
gridresults_r_all$Model &lt;- sapply(
  as.character(gridresults_r_all$Model), function(m) if (m == "null") "ZeroR" else m)

# Next, we want to order the models:
modelsFirst_r &lt;- c("ZeroR", "mmb:Simple", "mmb:Full")
modelsFirst_r &lt;- c(
  modelsFirst_r, sort(setdiff(unique(as.character(gridresults_r_all$Model)), modelsFirst_r)))
gridresults_r_all$Model &lt;- factor(
  as.character(gridresults_r_all$Model), levels = modelsFirst_r, ordered = TRUE)

tempDs &lt;- do.call(rbind, lapply(datasets_r, function(d) {
  return(do.call(rbind, lapply(c(caret_models_r, "Full", "Simple", "Naive"), function(m) {
    data &lt;- gridresults_r_all[gridresults_r_all$Model == m &amp; gridresults_r_all$Dataset == d, ]
    if (nrow(data) == 0) return(data.frame())
    return(data.frame(
      Model = m,
      Dataset = d,
      RMSE.mean = mean(data$RMSE),
      Rsquared.mean = mean(data$Rsquared),
      MAE.mean = mean(data$MAE)
    ))
  })))
}))

# So it prints at least a small blue bar indicating it is actually zero..
tempDs$Rsquared.mean &lt;- tempDs$Rsquared.mean + 1e-2

(ggplot(tempDs, aes(y = RMSE.mean, x = Model, fill = Model)) +
  labs(subtitle = "Comparison of RMSE.mean against the baseline (ZeroR) and other models.") +
  geom_bar(position = "dodge", stat = "identity") +
  coord_flip() +
  facet_wrap(Dataset ~., scales = "free", nrow = 2) +
  scale_color_brewer(palette = QPALETTE) +
  scale_fill_brewer(palette = QPALETTE) +
  theme_light(base_size = 9) +
  theme(text = element_text(family="Consolas"),
    axis.text.x = element_text(angle = 45, margin = margin(t=10), hjust = .66),
    axis.title.x = element_blank(), strip.background = element_rect(fill="#dfdfdf"),
    axis.title.y = element_text(margin = margin(r=10)),
    legend.text = element_text(size = 8),
    strip.text = element_text(color="black")) + labs(fill = "Method")
) %&gt;% saveAndPlotAsEPS("Bayes-all-datasets_RMSE", height = 12/2.54)</code></pre>
<p><img src="hyperparameters_files/figure-markdown_strict/unnamed-chunk-16-1.png"></p>
<pre><code>(ggplot(tempDs, aes(y = MAE.mean, x = Model, fill = Model)) +
  labs(subtitle = "Comparison of MAE.mean against the baseline (ZeroR) and other models.") +
  geom_bar(position = "dodge", stat = "identity") +
  coord_flip() +
  facet_wrap(Dataset ~., scales = "free", nrow = 2) +
  scale_color_brewer(palette = QPALETTE) +
  scale_fill_brewer(palette = QPALETTE) +
  theme_light(base_size = 9) +
  theme(text = element_text(family="Consolas"),
    axis.text.x = element_text(angle = 45, margin = margin(t=10), hjust = .66),
    axis.title.x = element_blank(), strip.background = element_rect(fill="#dfdfdf"),
    axis.title.y = element_text(margin = margin(r=10)),
    legend.text = element_text(size = 8),
    strip.text = element_text(color="black")) + labs(fill = "Method")
) %&gt;% saveAndPlotAsEPS("Bayes-all-datasets_MAE", height = 12/2.54)</code></pre>
<p><img src="hyperparameters_files/figure-markdown_strict/unnamed-chunk-16-2.png"></p>
<pre><code>(ggplot(tempDs, aes(y = Rsquared.mean, x = Model, fill = Model)) +
  labs(subtitle = "Comparison of (R^2).mean against the baseline (ZeroR) and other models.") +
  ylab("R^2 = cov^2") +
  geom_bar(position = "dodge", stat = "identity") +
  coord_flip() +
  facet_wrap(Dataset ~., scales = "free", nrow = 2) +
  scale_color_brewer(palette = QPALETTE) +
  scale_fill_brewer(palette = QPALETTE) +
  #scale_y_sqrt() +
  theme_light(base_size = 9) +
  theme(text = element_text(family="Consolas"),
    axis.text.x = element_text(angle = 45, margin = margin(t=10), hjust = .66),
    axis.title.x = element_blank(), strip.background = element_rect(fill="#dfdfdf"),
    axis.title.y = element_text(margin = margin(r=10)),
    legend.text = element_text(size = 8),
    strip.text = element_text(color="black")) + labs(fill = "Method")
) %&gt;% saveAndPlotAsEPS("Bayes-all-datasets_RSQ", height = 12/2.54)

## Warning: Removed 2 rows containing missing values (geom_bar).

## Warning: Removed 2 rows containing missing values (geom_bar).</code></pre>
<p><img src="hyperparameters_files/figure-markdown_strict/unnamed-chunk-16-3.png"></p>
<p>The above plots show only the mean of RMSE, R^2 and MAE. Let’s make a facetted boxplot for each of these metrics. For that, we need to transform and present the data in another shape.</p>
<pre><code># x-axis: dataset, y-axis: value, vertical-facet: model, horizontal-facet: metric
tempDs2 &lt;- data.frame(matrix(nrow = 0, ncol = 4))
colnames(tempDs2) &lt;- c("Dataset", "Value", "Model", "Metric")

temp &lt;- do.call(rbind, lapply(caret_models_r, function(m) {
  if (m == "bayesCaret") {
    # Let's subdivide this with simple=T/F
    tempS &lt;- gridresults_r_bc[gridresults_r_bc$hyper.mode == "simple", tempDsCols_r]
    tempS$Model &lt;- "Simple"
    tempF &lt;- gridresults_r_bc[gridresults_r_bc$hyper.mode == "full", tempDsCols_r]
    tempF$Model &lt;- "Full"
    
    return(rbind(tempS, tempF))
  }
  return(gridresults_r_others[, tempDsCols_r])
}))

for (ds in datasets_r) {
  for (mod in c("Simple", "Full", caret_models_r)) {
    for (met in c("RMSE", "Rsquared", "MAE")) {
      data &lt;- temp[temp$Model == mod, met]
      dLen &lt;- length(data)
      if (dLen == 0) next
      tempDs2 &lt;- rbind(tempDs2, data.frame(
        Dataset = rep(ds, dLen),
        Value = data,
        Model = rep(mod, dLen),
        Metric = rep(met, dLen)
      ))
    }
  }
}

# x-axis: dataset, y-axis: value, vertical-facet: model, horizontal-facet: metric
temp &lt;- ggplot(tempDs2, aes(x=Dataset, y=Value)) +
  geom_boxplot() +
  facet_grid(Metric ~ Model, scales = "free") +
  theme_light(base_size = 9) +
  theme(text = element_text(family="Consolas"))

#saveAndPlotAsEPS(temp, "Bayes-all-datasets-all-metrics", width = 1570, height = 1570)</code></pre>
<p>The previous plot is not terribly useful, unfortunately. Let’s show a facetted box-plot matrix per metric instead.</p>
<pre><code>facettedBoxplot(gridresults_r_all, "Rsquared", "R^2 (cov^2)") %&gt;% saveAndPlotAsEPS("Bayes-all-datasets_bp_RSQ", height = 3.2)

## Warning: Removed 56 rows containing non-finite values (stat_boxplot).

## Warning: Removed 56 rows containing non-finite values (stat_boxplot).</code></pre>
<p><img src="hyperparameters_files/figure-markdown_strict/unnamed-chunk-19-1.png"></p>
<pre><code>facettedBoxplot(gridresults_r_all, "RMSE", scales = "free_x", scale_y = scale_y_log10(
  breaks = scales::trans_breaks("log10", function(x) 10^x),
  labels = scales::trans_format("log10", scales::math_format(10^.x))
)) %&gt;% saveAndPlotAsEPS("Bayes-all-datasets_bp_RMSE", height = 3.2)</code></pre>
<p><img src="hyperparameters_files/figure-markdown_strict/unnamed-chunk-19-2.png"></p>
</div>
</div>
<div id="evaluation-of-the-effects-of-hyperparameters" class="section level2">
<h2 class="hasAnchor">
<a href="#evaluation-of-the-effects-of-hyperparameters" class="anchor"></a>Evaluation of the effects of Hyperparameters</h2>
<p>In this section, we will outline the effects of various hyperparameters in our Bayesian models. Since the type and amount of hyperparameters varies for classification and regression, we do them separately.</p>
<div id="classification-1" class="section level3">
<h3 class="hasAnchor">
<a href="#classification-1" class="anchor"></a>Classification</h3>
<p>In classification, we distinguish the three modes simple, full and naive. We have 4 hyperparameters (list below). For assessing the effect, we relate each hyperparameter’s realization to <em>Accuracy</em>. Accuracy and Kappa have a correlation coefficient of 0.74.</p>
<ul>
<li>
<code>shiftAmount</code>: Either <code>0</code> or <code>0.1</code>, so we could treat it as if it were boolean,</li>
<li>
<code>retainMinValues</code>: Integer between <code>0</code> and 101 (2, 4, 6, 8, 10, 11, 12, 20, 21, 26, 39, 47, 58, 101),</li>
<li>
<code>doEcdf</code>: Boolean,</li>
<li>
<code>online</code>: Integer between <code>0</code> (off) and 2147483647. Note that the maximum value is chosen so that all possible data is used. The values in use are: 0, 480, 607, 922, 1118, 1386, 2401, 2402, 2147483647.</li>
</ul>
<p>Per each of the three modes, we will print a facetted density plot.</p>
<pre><code>trainCounts &lt;- sapply(datasets_c, function(d) {
  return(nrow(get_dataset(d, 1)$train))
})
validCounts &lt;- sapply(datasets_c, function(d) {
  return(nrow(get_dataset(d, 1)$valid))
})

gridresults_c_bc$train.size &lt;- 0
for (n in names(trainCounts)) {
  gridresults_c_bc[gridresults_c_bc$Dataset == n, ]$train.size &lt;- trainCounts[[n]]
  
  # Where online == MAX, replace with actual max for dataset:
  theMax &lt;- trainCounts[[n]] + validCounts[[n]]
  temp &lt;- gridresults_c_bc[gridresults_c_bc$Dataset == n, ]$hyper.online
  temp &lt;- sapply(temp, function(v) if (v == .Machine$integer.max) theMax else v)
  gridresults_c_bc[gridresults_c_bc$Dataset == n, "hyper.onlineN"] &lt;- temp
}


tempDs_c &lt;- rbind(
  data.frame(
    Acc = gridresults_c_bc$Accuracy,
    Val = gridresults_c_bc$hyper.shiftAmount + 1e-10, # avoid 0
    Var = rep("shiftAmount", nrow(gridresults_c_bc)),
    doEcdf = gridresults_c_bc$hyper.doEcdf,
    Model = gridresults_c_bc$Model
  ),
  
  data.frame(
    Acc = gridresults_c_bc$Accuracy,
    Val = (gridresults_c_bc$hyper.retainMinValues / gridresults_c_bc$train.size) + 1e-10,
    Var = rep("retainMinValues", nrow(gridresults_c_bc)),
    doEcdf = gridresults_c_bc$hyper.doEcdf,
    Model = gridresults_c_bc$Model
  ),
  
  data.frame(
    Acc = gridresults_c_bc$Accuracy,
    Val = ifelse(gridresults_c_bc$hyper.online == 0,
                 gridresults_c_bc$Accuracy * max(sqrt(gridresults_c_bc$hyper.onlineN)),
                 sqrt(gridresults_c_bc$hyper.onlineN) + 1e-10),
    Var = rep("sqrt(online)", nrow(gridresults_c_bc)),
    doEcdf = gridresults_c_bc$hyper.doEcdf,
    Model = gridresults_c_bc$Model
  )
)

facettedDensityPlot &lt;- function(ds, metric, metricName = metric, form, subtitle, scale_y = scale_y_continuous(), labeller = label_both, grad_high = "#102723", grad_low = "#8DD3C7") {
  ggplot(ds, aes(x=Val, y=ds[[metric]])) +
    stat_density_2d(aes(fill = ..level..), geom="polygon", color = "white") +
    facet_grid(form, scales = "free", labeller = labeller) +
    theme_light() +
    theme(plot.subtitle = element_text(size = 7, margin = margin(b=5)),
          text = element_text(family="Consolas"),
          axis.text.x = element_text(
            angle = 90,
            margin = margin(t=5),
            hjust = 1,
            vjust = .4
          ),
          axis.title.x = element_blank(),
          axis.title.y = element_text(margin = margin(r=10), size = 7),
          strip.background = element_rect(fill="#dfdfdf"),
          strip.text = element_text(color="black", size = 8),
          legend.text = element_text(size = 7),
          legend.title = element_text(
            angle = 270,
            margin = margin(b=5),
            size = 8,
            vjust = .15
          )
    ) +
    scale_y +
    scale_fill_gradient(high = grad_high, low = grad_low) +
    labs(
      fill = "Relative\nLikelihood",
      subtitle = subtitle
    ) +
    ylab(metricName)
}

facettedDensityPlot(
  ds = tempDs_c[tempDs_c$Model == "bc_s",],
  metric = "Acc",
  metricName = "Accuracy",
  form = doEcdf ~ Var,
  subtitle = "Effect of Hyperparameters in simple Bayesian classification."
) %&gt;% saveAndPlotAsEPS("Bayes-hps-c-simple", height = 3.2)</code></pre>
<p><img src="hyperparameters_files/figure-markdown_strict/unnamed-chunk-22-1.png"></p>
<pre><code>facettedDensityPlot(
  ds = tempDs_c[tempDs_c$Model == "bc_n",],
  metric = "Acc",
  metricName = "Accuracy",
  form = doEcdf ~ Var,
  subtitle = "Effect of Hyperparameters in naive Bayesian classification."
) %&gt;% saveAndPlotAsEPS("Bayes-hps-c-naive", height = 3.2)</code></pre>
<p><img src="hyperparameters_files/figure-markdown_strict/unnamed-chunk-22-2.png"></p>
<pre><code>facettedDensityPlot(
  ds = tempDs_c[tempDs_c$Model == "bc_f",],
  metric = "Acc",
  metricName = "Accuracy",
  form = doEcdf ~ Var,
  subtitle = "Effect of Hyperparameters in full Bayesian classification."
) %&gt;% saveAndPlotAsEPS("Bayes-hps-c-full", height = 3.2)</code></pre>
<p><img src="hyperparameters_files/figure-markdown_strict/unnamed-chunk-22-3.png"></p>
</div>
<div id="regression-1" class="section level3">
<h3 class="hasAnchor">
<a href="#regression-1" class="anchor"></a>Regression</h3>
<pre><code>trainCounts_r &lt;- sapply(datasets_r, function(d) {
  return(nrow(get_dataset(d, 1)$train))
})
validCounts_r &lt;- sapply(datasets_r, function(d) {
  return(nrow(get_dataset(d, 1)$valid))
})

gridresults_r_bc$train.size &lt;- 0
for (n in names(trainCounts_r)) {
  gridresults_r_bc[gridresults_r_bc$Dataset == n, ]$train.size &lt;- trainCounts_r[[n]]
  
  # Where online == MAX, replace with actual max for dataset:
  theMax &lt;- trainCounts_r[[n]] + validCounts_r[[n]]
  temp &lt;- gridresults_r_bc[gridresults_r_bc$Dataset == n, ]$hyper.online
  temp &lt;- sapply(temp, function(v) if (v == .Machine$integer.max) theMax else v)
  gridresults_r_bc[gridresults_r_bc$Dataset == n, "hyper.onlineN"] &lt;- temp
}

gridresults_r_bc$hyper.numBucketsN &lt;- gridresults_r_bc$hyper.numBuckets
gridresults_r_bc[is.na(gridresults_r_bc$hyper.numBuckets),]$hyper.numBucketsN &lt;-
  ceiling(log2(gridresults_r_bc[is.na(gridresults_r_bc$hyper.numBuckets),]$train.size))

gridresults_r_bc$Category &lt;- ""
for (doEcdf in c(TRUE, FALSE)) {
  for (sampleAll in c(TRUE, FALSE)) {
    theCat &lt;- paste("ecdf:", if (doEcdf) "T" else "F", ", s.A.:", if (sampleAll) "T" else "F", sep = "")
    gridresults_r_bc[gridresults_r_bc$hyper.doEcdf == doEcdf &amp; gridresults_r_bc$hyper.sampleFromAllBuckets == sampleAll, ]$Category &lt;- theCat
  }
}


tempDs_r &lt;- rbind(
  data.frame(
    Rsq = gridresults_r_bc$Rsquared,
    Val = gridresults_r_bc$hyper.shiftAmount + runif(nrow(gridresults_r_bc)) * 1e-10, # avoid 0
    Var = rep("shiftAmount", nrow(gridresults_r_bc)),
    Cat = gridresults_r_bc$Category,
    Model = gridresults_r_bc$Model
  ),
  
  data.frame(
    Rsq = gridresults_r_bc$Rsquared,
    Val = (gridresults_r_bc$hyper.retainMinValues / gridresults_r_bc$train.size) + 1e-10,
    Var = rep("retainMinValues", nrow(gridresults_r_bc)),
    Cat = gridresults_r_bc$Category,
    Model = gridresults_r_bc$Model
  ),
  
  data.frame(
    Rsq = gridresults_r_bc$Rsquared,
    Val = gridresults_r_bc$hyper.numBucketsN,
    Var = rep("numBuckets", nrow(gridresults_r_bc)),
    Cat = gridresults_r_bc$Category,
    Model = gridresults_r_bc$Model
  ),
  
  data.frame(
    Rsq = gridresults_r_bc$Rsquared,
    Val = ifelse(gridresults_r_bc$hyper.online == 0,
                 gridresults_r_bc$Rsquared * max(sqrt(gridresults_r_bc$hyper.onlineN)),
                 sqrt(gridresults_r_bc$hyper.onlineN) + 1e-10),
    Var = rep("sqrt(online)", nrow(gridresults_r_bc)),
    Cat = gridresults_r_bc$Category,
    Model = gridresults_r_bc$Model
  )
)

facettedDensityPlot(
  ds = tempDs_r[tempDs_r$Model == "bc_s",],
  metric = "Rsq",
  metricName = "R^2 = cov^2",
  form = Cat ~ Var,
  subtitle ="Effect of Hyperparameters in simple Bayesian regression.",
  grad_high = "#2C2749",
  grad_low = "#BEBADA",
  scale_y = scale_y_sqrt()
) %&gt;% saveAndPlotAsEPS("Bayes-hps-r-simple")</code></pre>
<p><img src="hyperparameters_files/figure-markdown_strict/unnamed-chunk-24-1.png"></p>
<pre><code>facettedDensityPlot(
  ds = tempDs_r[tempDs_r$Model == "bc_f",],
  metric = "Rsq",
  metricName = "R^2 = cov^2",
  form = Cat ~ Var,
  subtitle ="Effect of Hyperparameters in full Bayesian regression.",
  grad_high = "#2C2749",
  grad_low = "#BEBADA",
  scale_y = scale_y_sqrt()
) #%&gt;% saveAndPlotAsEPS("Bayes-hps-r-full")</code></pre>
<p><img src="hyperparameters_files/figure-markdown_strict/unnamed-chunk-24-2.png"></p>
<p>As can be seen in the above plot, we do not get great visual results for <code>shiftAmount</code> if <code>sampleFromAllBuckets=TRUE</code>, so let’s plot it again w/o this column:</p>
<pre><code>facettedDensityPlot(
  ds = tempDs_r[tempDs_r$Model == "bc_f" &amp; tempDs_r$Var != "shiftAmount",],
  metric = "Rsq",
  metricName = "R^2 = cov^2",
  form = Cat ~ Var,
  subtitle ="Effect of Hyperparameters in full Bayesian regression.",
  scale_y = scale_y_sqrt(),
  grad_high = "#2C2749",
  grad_low = "#BEBADA",
  labeller = labeller(Cat = c(
    "ecdf:F, s.A.:F" = "ecdf:F, sA:F",
    "ecdf:F, s.A.:T" = "ecdf:F, sA:T",
    "ecdf:T, s.A.:F" = "ecdf:T, sA:F",
    "ecdf:T, s.A.:T" = "ecdf:T, sA:T"
  ))
) %&gt;% saveAndPlotAsEPS("Bayes-hps-r-full", height = 4.6)</code></pre>
<p><img src="hyperparameters_files/figure-markdown_strict/unnamed-chunk-25-1.png"></p>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">

        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li>
<a href="#grid-search">Grid search</a><ul class="nav nav-pills nav-stacked">
<li><a href="#classification">Classification</a></li>
      <li><a href="#regression">Regression</a></li>
      </ul>
</li>
      <li>
<a href="#evaluation">Evaluation</a><ul class="nav nav-pills nav-stacked">
<li><a href="#evaluation-of-the-bayesian-models-compared-to-other-models">Evaluation of the Bayesian models, compared to other models</a></li>
      <li><a href="#evaluation-of-the-effects-of-hyperparameters">Evaluation of the effects of Hyperparameters</a></li>
      </ul>
</li>
      </ul>
</div>
      </div>

</div>



      <footer><div class="copyright">
  <p>Developed by Sebastian Hönel.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.4.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
